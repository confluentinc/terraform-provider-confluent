output "instructions" {
  value = <<-EOT
  Environment ID:   ${data.confluent_environment.staging.id}
  Kafka Cluster ID: ${confluent_kafka_cluster.enterprise.id}
  Kafka topic name: ${local.topic_name}

  Service Accounts and their Kafka API Keys (API Keys inherit the permissions granted to the owner):
  ${confluent_service_account.app-manager.display_name}:                     ${confluent_service_account.app-manager.id}
  ${confluent_service_account.app-manager.display_name}'s Kafka API Key:     "${confluent_api_key.app-manager-kafka-api-key.id}"
  ${confluent_service_account.app-manager.display_name}'s Kafka API Secret:  "${confluent_api_key.app-manager-kafka-api-key.secret}"

  ${confluent_service_account.app-producer.display_name}:                    ${confluent_service_account.app-producer.id}
  ${confluent_service_account.app-producer.display_name}'s Kafka API Key:    "${confluent_api_key.app-producer-kafka-api-key.id}"
  ${confluent_service_account.app-producer.display_name}'s Kafka API Secret: "${confluent_api_key.app-producer-kafka-api-key.secret}"

  ${confluent_service_account.app-consumer.display_name}:                    ${confluent_service_account.app-consumer.id}
  ${confluent_service_account.app-consumer.display_name}'s Kafka API Key:    "${confluent_api_key.app-consumer-kafka-api-key.id}"
  ${confluent_service_account.app-consumer.display_name}'s Kafka API Secret: "${confluent_api_key.app-consumer-kafka-api-key.secret}"

  üîë SSH SETUP INSTRUCTIONS:

  1. First, save your private key:
     echo '${tls_private_key.main.private_key_pem}' > ~/.ssh/pni-test-key.pem

  2. Set correct permissions on the private key:
     chmod 600 ~/.ssh/pni-test-key.pem

  3. Verify the key file permissions and content:
     ls -la ~/.ssh/pni-test-key.pem
     head -n 1 ~/.ssh/pni-test-key.pem  # Should show "-----BEGIN RSA PRIVATE KEY-----" or similar

  4. Connect to your EC2 instance:
     ssh -i ~/.ssh/pni-test-key.pem ec2-user@${aws_instance.test.public_ip}

  5. If you get permission errors, try these alternatives:
     # Option A: Disable strict host key checking
     ssh -i ~/.ssh/pni-test-key.pem -o StrictHostKeyChecking=no ec2-user@${aws_instance.test.public_ip}

     # Option B: Enable verbose mode to debug connection issues
     ssh -i ~/.ssh/pni-test-key.pem -v ec2-user@${aws_instance.test.public_ip}

     # Option C: Test with different user (in case ec2-user is not correct)
     ssh -i ~/.ssh/pni-test-key.pem -o StrictHostKeyChecking=no admin@${aws_instance.test.public_ip}

  üîß TROUBLESHOOTING:

  If SSH connection fails with "Permission denied (publickey)", check:
  - Key file permissions: chmod 600 ~/.ssh/pni-test-key.pem
  - Key file format: The key should start with "-----BEGIN" and end with "-----END"
  - Security group allows SSH (port 22) from your IP
  - Instance is running and has a public IP
  - The correct username for Amazon Linux 2 is typically 'ec2-user'

  üìù Note: The private key is automatically generated by Terraform.

  üèóÔ∏è TERRAFORM SETUP INSTRUCTIONS:

  1. Create main.tf file on your EC2 instance with the following content:

     cat << 'MAINEOF' > main.tf
terraform {
  required_version = ">= 0.14.0"
  required_providers {
    confluent = {
      source  = "confluentinc/confluent"
      version = "2.32.0"
    }
  }
}

provider "confluent" {
  kafka_id            = "${confluent_kafka_cluster.enterprise.id}"
  kafka_rest_endpoint = "${[for endpoint in confluent_kafka_cluster.enterprise.endpoints : endpoint.rest_endpoint if endpoint.access_point_id == confluent_access_point.aws.id][0]}"
  kafka_api_key       = "${confluent_api_key.app-manager-kafka-api-key.id}"
  kafka_api_secret    = "${confluent_api_key.app-manager-kafka-api-key.secret}"
}

resource "confluent_kafka_topic" "orders" {
  topic_name    = "${local.topic_name}"
  lifecycle {
    prevent_destroy = true
  }
}
MAINEOF
  2. Initialize and apply Terraform:
     terraform init
     terraform apply

  üß™ TESTING COMMANDS (run these after SSH into EC2):

  # First, check if user_data script ran successfully
  sudo cat /var/log/cloud-init-output.log | tail -20  # Check for any errors during boot

  # If Confluent CLI is not installed, install it manually using binary download (YUM package requires newer GLIBC):

  # Download and install Confluent CLI binary directly (compatible with Amazon Linux 2)
  curl -sL --http1.1 https://cnfl.io/cli | sudo sh -s -- -b /usr/local/bin

  # Alternative: Download specific version manually
  # wget https://packages.confluent.io/cli/confluent-cli_latest_linux_amd64.tar.gz
  # tar -xzf confluent-cli_latest_linux_amd64.tar.gz
  # sudo mv confluent /usr/local/bin/
  # sudo chmod +x /usr/local/bin/confluent

  # Verify Confluent CLI installation
  confluent version

  # If you need Terraform on the EC2 instance, install it:
  sudo yum install -y yum-utils
  sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
  sudo yum -y install terraform

  # Verify Terraform installation
  terraform version

  # Test connectivity to the Kafka cluster REST endpoint that corresponds to the target access point
  curl --request GET \
  --url ${[for endpoint in confluent_kafka_cluster.enterprise.endpoints : endpoint.rest_endpoint if endpoint.access_point_id == confluent_access_point.aws.id][0]}/kafka/v3/clusters/${confluent_kafka_cluster.enterprise.id}/topics \
  -u "${confluent_api_key.app-manager-kafka-api-key.id}:${confluent_api_key.app-manager-kafka-api-key.secret}"

  In order to use the Confluent CLI v4 to produce and consume messages from topic '${local.topic_name}' using Kafka API Keys
  of ${confluent_service_account.app-producer.display_name} and ${confluent_service_account.app-consumer.display_name} service accounts
  run the following commands:

  # 1. Log in to Confluent Cloud
  $ confluent login

  # 2. Produce key-value records to topic '${local.topic_name}' by using ${confluent_service_account.app-producer.display_name}'s Kafka API Key
  $ confluent kafka topic produce ${local.topic_name} --environment ${data.confluent_environment.staging.id} --cluster ${confluent_kafka_cluster.enterprise.id} --api-key "${confluent_api_key.app-producer-kafka-api-key.id}" --api-secret "${confluent_api_key.app-producer-kafka-api-key.secret} --bootstrap ${confluent_kafka_cluster.enterprise.bootstrap_endpoint}"
  # Enter a few records and then press 'Ctrl-C' when you're done.
  # Sample records:
  # {"number":1,"date":18500,"shipping_address":"899 W Evelyn Ave, Mountain View, CA 94041, USA","cost":15.00}
  # {"number":2,"date":18501,"shipping_address":"1 Bedford St, London WC2E 9HG, United Kingdom","cost":5.00}
  # {"number":3,"date":18502,"shipping_address":"3307 Northland Dr Suite 400, Austin, TX 78731, USA","cost":10.00}

  # 3. Consume records from topic '${local.topic_name}' by using ${confluent_service_account.app-consumer.display_name}'s Kafka API Key
  $ confluent kafka topic consume ${local.topic_name} --from-beginning --environment ${data.confluent_environment.staging.id} --cluster ${confluent_kafka_cluster.enterprise.id} --api-key "${confluent_api_key.app-consumer-kafka-api-key.id}" --api-secret "${confluent_api_key.app-consumer-kafka-api-key.secret} --bootstrap ${confluent_kafka_cluster.enterprise.bootstrap_endpoint}"
  # When you are done, press 'Ctrl-C'.
  EOT

  sensitive = true
}
