---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "confluent_tableflow_topic Data Source - terraform-provider-confluent"
subcategory: ""
description: |-
  
---

# confluent_tableflow_topic Data Source

[![General Availability](https://img.shields.io/badge/Lifecycle%20Stage-General%20Availability-%2345c6e8)](https://docs.confluent.io/cloud/current/api.html#section/Versioning/API-Lifecycle-Policy)

`confluent_tableflow_topic` describes a Tableflow Topic data source.

## Example Usage

### Option #1: Manage multiple Tableflow Topics in the same Terraform workspace

```terraform
provider "confluent" {
  cloud_api_key    = var.confluent_cloud_api_key    # optionally use CONFLUENT_CLOUD_API_KEY env var
  cloud_api_secret = var.confluent_cloud_api_secret # optionally use CONFLUENT_CLOUD_API_SECRET env var
}

data "confluent_tableflow_topic" "example" {
  environment {
    id = data.confluent_environment.staging.id
  }
  kafka_cluster {
    id = data.confluent_kafka_cluster.staging.id
  }
  display_name = "tableflow-example"
  credentials {
    key    = confluent_api_key.env-admin-tableflow-api-key.id
    secret = confluent_api_key.env-admin-tableflow-api-key.secret
  }
}

output "retention-ms" {
  value = data.confluent_tableflow_topic.example.retention_ms
}
```

### Option #2: Manage a single Tableflow Topic in the same Terraform workspace

```terraform
provider "confluent" {
  cloud_api_key        = var.confluent_cloud_api_key    # optionally use CONFLUENT_CLOUD_API_KEY env var
  cloud_api_secret     = var.confluent_cloud_api_secret # optionally use CONFLUENT_CLOUD_API_SECRET env var
  tableflow_api_key    = var.tableflow_api_key          # optionally use TABLEFLOW_API_KEY env var
  tableflow_api_secret = var.tableflow_api_secret       # optionally use TABLEFLOW_API_SECRET env var
}

data "confluent_tableflow_topic" "example" {
  display_name = "tableflow-example"
}

output "retention-ms" {
  value = data.confluent_tableflow_topic.example.retention_ms
}
```

<!-- schema generated by tfplugindocs -->
## Argument Reference

The following arguments are supported:

- `environment` (Required Configuration Block) supports the following:
    - `id` - (Required String) The ID of the Environment, for example, `env-abc123`. 
- `kafka_cluster` (Required Configuration Block) supports the following:
    - `id` - (Required String) The ID of the Kafka cluster, for example, `lkc-abc123`.
- `display_name` - (Required String) The name of the Tableflow Topic.
- `credentials` (Optional Configuration Block) supports the following:
    - `key` - (Required String) The Tableflow API Key.
    - `secret` - (Required String) The Tableflow API Secret.

-> **Note:** A Tableflow API key consists of a key and a secret. Tableflow API keys are required to interact with Tableflow Topics in Confluent Cloud.

!> **Warning:** Use Option #2 to avoid exposing sensitive `credentials` value in a state file. When using Option #1, Terraform doesn't encrypt the sensitive `credentials` value of the `confluent_tableflow_topic` data source, so you must keep your state file secure to avoid exposing it. Refer to the [Terraform documentation](https://www.terraform.io/docs/language/state/sensitive-data.html) to learn more about securing your state file.

## Attributes Reference

In addition to the preceding arguments, the following attributes are exported:

- `retention_ms` - (Optional String) The max age of snapshots (Iceberg) or versions (Delta) (snapshot/version expiration) to keep on the table in milliseconds for the Tableflow enabled topic.
- `table_formats` - (Optional List) The supported table formats for the Tableflow-enabled topic.
- `table_path` - (Optional String) The current storage path where the data and metadata is stored for this table.
- `record_failure_strategy` - (Optional String, **Deprecated**) The strategy to handle record failures in the Tableflow enabled topic during materialization. For `SKIP`, we skip the bad records and move to the next record. For `SUSPEND`, we suspend the materialization of the topic.
- `error_handling` (Optional Configuration Block) supports the following:
    - `mode` - (Optional String) The error handling mode. For `SUSPEND`, the materialization of the topic is suspended in case of record failures. For `SKIP`, bad records are skipped and the materialization continues with the next record. For `LOG`, bad records are logged to a dead-letter queue (DLQ) topic and the materialization continues with the next record. The default mode is `SUSPEND`.
    - `log_target` - (Optional String) The topic to which the bad records will be logged for error handling mode `LOG`. The default topic is "error_log" if error handling mode is `LOG`, and empty otherwise.
- `enable_compaction` - (Optional Boolean) This flag determines whether to enable compaction for the Tableflow enabled topic.
- `enable_partitioning` - (Optional Boolean) This flag determines whether to enable partitioning for the Tableflow enabled topic.
- `suspended` - (Optional Boolean) Indicates whether the Tableflow should be suspended.
- `write_mode` - (Optional String) Indicates the write mode of the Tableflow topic.
- `byob_aws` (Optional Configuration Block) supports the following:
    - `bucket_name` - (Required String) The bucket name.
    - `bucket_region` - (Required String) The bucket region.
    - `provider_integration_id` - (Required String) The provider integration id.
- `azure_data_lake_storage_gen_2` (Optional Configuration Block) supports the following:
    - `container_name` - (Required String) The container name.
    - `storage_account_name` - (Required String) The storage account name.
    - `provider_integration_id` - (Required String) The provider integration id.
    - `storage_region` - (Required String) The storage region.

