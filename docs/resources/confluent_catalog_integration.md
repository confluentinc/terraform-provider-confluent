---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "confluent_catalog_integration Resource - terraform-provider-confluent"
subcategory: ""
description: |-
  
---

# confluent_catalog_integration Resource

[![General Availability](https://img.shields.io/badge/Lifecycle%20Stage-General%20Availability-%2345c6e8)](https://docs.confluent.io/cloud/current/api.html#section/Versioning/API-Lifecycle-Policy)

-> **Note:** It is recommended to set `lifecycle { prevent_destroy = true }` on production instances to prevent accidental catalog integration deletion. This setting rejects plans that would destroy or recreate the catalog integration, such as attempting to change uneditable attributes. Read more about it in the [Terraform docs](https://www.terraform.io/language/meta-arguments/lifecycle#prevent_destroy).

## Example Usage

### Option #1: Manage multiple Catalog Integrations in the same Terraform workspace

```terraform
provider "confluent" {
  cloud_api_key    = var.confluent_cloud_api_key    # optionally use CONFLUENT_CLOUD_API_KEY env var
  cloud_api_secret = var.confluent_cloud_api_secret # optionally use CONFLUENT_CLOUD_API_SECRET env var
}

resource "confluent_catalog_integration" "example" {
  environment {
    id = data.confluent_environment.staging.id
  }
  kafka_cluster {
    id = data.confluent_kafka_cluster.staging.id
  }
  display_name = "catalog-integration-1"
  catalog_integration_aws_glue {
    provider_integration_id = data.confluent_provider_integration.main.id
  }
  credentials {
    key    = confluent_api_key.env-admin-tableflow-api-key.id
    secret = confluent_api_key.env-admin-tableflow-api-key.secret
  }

  lifecycle {
    prevent_destroy = true
  }
}
```

### Option #2: Manage a single Catalog Integration in the same Terraform workspace

```terraform
provider "confluent" {
  cloud_api_key        = var.confluent_cloud_api_key    # optionally use CONFLUENT_CLOUD_API_KEY env var
  cloud_api_secret     = var.confluent_cloud_api_secret # optionally use CONFLUENT_CLOUD_API_SECRET env var
  tableflpw_api_key    = var.tableflow_api_key          # optionally use TABLEFLOW_API_KEY env var
  tableflow_api_secret = var.tableflow_api_secret       # optionally use TABLEFLOW_API_SECRET env var
}

resource "confluent_catalog_integration" "example" {
  environment {
    id = data.confluent_environment.staging.id
  }
  kafka_cluster {
    id = data.confluent_kafka_cluster.staging.id
  }
  display_name = "catalog-integration-1"
  catalog_integration_snowflake {
      endpoint = "https://vuser1_polaris.snowflakecomputing.com/"
      client_id = "client-id"
      client_secret = "***REDACTED***"
      warehouse = "catalog-name"
      allowed_scope = "session:role:R1"
  }

  lifecycle {
    prevent_destroy = true
  }
}
```

<!-- schema generated by tfplugindocs -->
## Argument Reference

The following arguments are supported:

- `environment` (Required Configuration Block) supports the following:
    - `id` - (Required String) The ID of the Environment, for example, `env-abc123`. 
- `kafka_cluster` (Required Configuration Block) supports the following:
    - `id` - (Required String) The ID of the Kafka cluster, for example, `lkc-abc123`.
- `display_name` - (Required String) The name of the catalog integration.
- `catalog_integration_aws_glue` (Optional Configuration Block) supports the following:
    - `provider_integration_id` - (Required String) The provider integration id.
- `catalog_integration_snowflake` (Optional Configuration Block) supports the following:
    - `endpoint` - (Required String) The catalog integration connection endpoint for Snowflake Open Catalog.
    - `client_id` - (Required String, Sensitive) The client ID of the catalog integration.
    - `client_secret` - (Required String, Sensitive) The client secret of the catalog integration.
    - `warehouse` - (Required String) Warehouse name of the Snowflake Open Catalog.
    - `allowed_scope` - (Required String) Allowed scope of the Snowflake Open Catalog.
- `credentials` (Optional Configuration Block) supports the following:
    - `key` - (Required String) The Tableflow API Key.
    - `secret` - (Required String, Sensitive) The Tableflow API Secret.

-> **Note:** A Tableflow API key consists of a key and a secret. Tableflow API keys are required to interact with Catalog Integrations in Confluent Cloud.

-> **Note:** Use Option #2 to simplify the key rotation process. When using Option #1, to rotate a Tableflow API key, create a new Tableflow API key, update the `credentials` block in all configuration files to use the new Tableflow API key, run `terraform apply -target="confluent_catalog_integration.example"`, and remove the old Tableflow API key. Alternatively, in case the old Tableflow API Key was deleted already, you might need to run `terraform plan -refresh=false -target="confluent_catalog_integration.example" -out=rotate-tableflow-api-key` and `terraform apply rotate-tableflow-api-key` instead.

!> **Warning:** Use Option #2 to avoid exposing sensitive `credentials` value in a state file. When using Option #1, Terraform doesn't encrypt the sensitive `credentials` value of the `confluent_catalog_integration` resource, so you must keep your state file secure to avoid exposing it. Refer to the [Terraform documentation](https://www.terraform.io/docs/language/state/sensitive-data.html) to learn more about securing your state file.

## Attributes Reference

In addition to the preceding arguments, the following attributes are exported:

- `id` - (Required String) The ID of the Catalog Integration, for example, `tci-abc123`.
- `suspended` - (Optional Boolean) Indicates whether the Catalog Integration should be suspended.
- `last_sync_at` - (Optional String) The date and time at which the catalog was last synced. It is represented in RFC3339 format and is in UTC.

## Import

You can import a Catalog Integration by using the Catalog Integration name, Environment ID, and Kafka Cluster ID, in the format `<Environment ID>/<Kafka Cluster ID>/<Catalog Integration Id>`, for example:

```shell
# Option #1: Manage multiple Catalog Integrations in the same Terraform workspace
$ export IMPORT_TABLEFLOW_API_KEY="<tableflow_api_key>"
$ export IMPORT_TABLEFLOW_API_SECRET="<tableflow_api_secret>"
$ terraform import confluent_catalog_integration.example env-abc123/lkc-abc123/tci-abc123

# Option #2: Manage a single Catalog Integration in the same Terraform workspace
$ terraform import confluent_catalog_integration.example env-abc123/lkc-abc123/tci-abc123
```

!> **Warning:** Do not forget to delete terminal command history afterwards for security purposes.
